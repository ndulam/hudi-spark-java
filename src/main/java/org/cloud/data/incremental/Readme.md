1. Create table and insert two records. 

2. Read table data. Record stamp as T1 --> 20231125064757325
   +-------------------+---------------------+------------------+----------------------+------------------------------------------------------------------------+-----------+-----------+-------------+
   |_hoodie_commit_time|_hoodie_commit_seqno |_hoodie_record_key|_hoodie_partition_path|_hoodie_file_name                                                       |ts         |employee_id|employee_name|
   +-------------------+---------------------+------------------+----------------------+------------------------------------------------------------------------+-----------+-----------+-------------+
   |20231125064757325  |20231125064757325_0_0|1                 |                      |88e4e8ee-f48d-4ab1-bd11-ff6745ac4ba9-0_0-25-20_20231125064757325.parquet|1700740753L|1          |Naresh Dulam1|
   |20231125064757325  |20231125064757325_0_1|2                 |                      |88e4e8ee-f48d-4ab1-bd11-ff6745ac4ba9-0_0-25-20_20231125064757325.parquet|1700740753L|2          |Naresh Dulam2|
   +-------------------+---------------------+------------------+----------------------+------------------------------------------------------------------------+-----------+-----------+-------------+

3. Add row and update row. Record timestamp as T2 --> 20231125064846585
   +-------------------+---------------------+------------------+----------------------+------------------------------------------------------------------------+-----------+-----------+---------------+
   |_hoodie_commit_time|_hoodie_commit_seqno |_hoodie_record_key|_hoodie_partition_path|_hoodie_file_name                                                       |ts         |employee_id|employee_name  |
   +-------------------+---------------------+------------------+----------------------+------------------------------------------------------------------------+-----------+-----------+---------------+
   |20231125064846585  |20231125064846585_0_0|1                 |                      |88e4e8ee-f48d-4ab1-bd11-ff6745ac4ba9-0_0-15-11_20231125064846585.parquet|1700740961L|1          |Naresh Dulam1.1|
   |20231125064757325  |20231125064757325_0_1|2                 |                      |88e4e8ee-f48d-4ab1-bd11-ff6745ac4ba9-0_0-15-11_20231125064846585.parquet|1700740753L|2          |Naresh Dulam2  |
   |20231125064846585  |20231125064846585_0_2|3                 |                      |88e4e8ee-f48d-4ab1-bd11-ff6745ac4ba9-0_0-15-11_20231125064846585.parquet|1700740961L|3          |Naresh Dulam3  |
   +-------------------+---------------------+------------------+----------------------+------------------------------------------------------------------------+-----------+-----------+---------------+

4. Add new row. Record time stamp as T3 --> 20231125065035520
   +-------------------+---------------------+------------------+----------------------+------------------------------------------------------------------------+-----------+-----------+---------------+
   |_hoodie_commit_time|_hoodie_commit_seqno |_hoodie_record_key|_hoodie_partition_path|_hoodie_file_name                                                       |ts         |employee_id|employee_name  |
   +-------------------+---------------------+------------------+----------------------+------------------------------------------------------------------------+-----------+-----------+---------------+
   |20231125064846585  |20231125064846585_0_0|1                 |                      |88e4e8ee-f48d-4ab1-bd11-ff6745ac4ba9-0_0-15-11_20231125065035520.parquet|1700740961L|1          |Naresh Dulam1.1|
   |20231125064757325  |20231125064757325_0_1|2                 |                      |88e4e8ee-f48d-4ab1-bd11-ff6745ac4ba9-0_0-15-11_20231125065035520.parquet|1700740753L|2          |Naresh Dulam2  |
   |20231125064846585  |20231125064846585_0_2|3                 |                      |88e4e8ee-f48d-4ab1-bd11-ff6745ac4ba9-0_0-15-11_20231125065035520.parquet|1700740961L|3          |Naresh Dulam3  |
   |20231125065035520  |20231125065035520_0_3|4                 |                      |88e4e8ee-f48d-4ab1-bd11-ff6745ac4ba9-0_0-15-11_20231125065035520.parquet|1700740961L|4          |Naresh Dulam4  |
   +-------------------+---------------------+------------------+----------------------+------------------------------------------------------------------------+-----------+-----------+---------------+
5. remove row. No timestamp as record deleted , reading table not give any timestamp

+-------------------+---------------------+------------------+----------------------+------------------------------------------------------------------------+-----------+-----------+---------------+
|_hoodie_commit_time|_hoodie_commit_seqno |_hoodie_record_key|_hoodie_partition_path|_hoodie_file_name                                                       |ts         |employee_id|employee_name  |
+-------------------+---------------------+------------------+----------------------+------------------------------------------------------------------------+-----------+-----------+---------------+
|20231125064846585  |20231125064846585_0_0|1                 |                      |88e4e8ee-f48d-4ab1-bd11-ff6745ac4ba9-0_0-20-13_20231125065711523.parquet|1700740961L|1          |Naresh Dulam1.1|
|20231125064846585  |20231125064846585_0_2|3                 |                      |88e4e8ee-f48d-4ab1-bd11-ff6745ac4ba9-0_0-20-13_20231125065711523.parquet|1700740961L|3          |Naresh Dulam3  |
|20231125065035520  |20231125065035520_0_3|4                 |                      |88e4e8ee-f48d-4ab1-bd11-ff6745ac4ba9-0_0-20-13_20231125065711523.parquet|1700740961L|4          |Naresh Dulam4  |
+-------------------+---------------------+------------------+----------------------+------------------------------------------------------------------------+-----------+-----------+---------------+ 

6. Read incremental data . Read table data between **after** T1(20231125064757325) and T2(20231125064846585) **include**
   Dataset employee =spark.read().format("org.apache.hudi").option("hoodie.datasource.query.type", "incremental")
   .option("hoodie.datasource.read.begin.instanttime", "20231125064757325")
   .option("hoodie.datasource.read.end.instanttime", "20231125064846585")
   .load("file:///D:/sparksetup/sparkdata/employee_table");
   employee.show();

+-------------------+--------------------+------------------+----------------------+--------------------+-----------+-----------+---------------+
|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|         ts|employee_id|  employee_name|
+-------------------+--------------------+------------------+----------------------+--------------------+-----------+-----------+---------------+
|  20231125064846585|20231125064846585...|                 1|                      |88e4e8ee-f48d-4ab...|1700740961L|          1|Naresh Dulam1.1|
|  20231125064846585|20231125064846585...|                 3|                      |88e4e8ee-f48d-4ab...|1700740961L|          3|  Naresh Dulam3|
+-------------------+--------------------+------------------+----------------------+--------------------+-----------+-----------+---------------+

7. Read incremental data . Read table data between **after** 0  and T1(20231125064757325) **include**
    Dataset employee1 =spark.read().format("org.apache.hudi").option("hoodie.datasource.query.type", "incremental")
                .option("hoodie.datasource.read.begin.instanttime", "0")
                .option("hoodie.datasource.read.end.instanttime", "20231125064757325")
                .load("file:///D:/sparksetup/sparkdata/employee_table");

+-------------------+--------------------+------------------+----------------------+--------------------+-----------+-----------+-------------+
|_hoodie_commit_time|_hoodie_commit_seqno|_hoodie_record_key|_hoodie_partition_path|   _hoodie_file_name|         ts|employee_id|employee_name|
+-------------------+--------------------+------------------+----------------------+--------------------+-----------+-----------+-------------+
|  20231125064757325|20231125064757325...|                 1|                      |88e4e8ee-f48d-4ab...|1700740753L|          1|Naresh Dulam1|
|  20231125064757325|20231125064757325...|                 2|                      |88e4e8ee-f48d-4ab...|1700740753L|          2|Naresh Dulam2|
+-------------------+--------------------+------------------+----------------------+--------------------+-----------+-----------+-------------+